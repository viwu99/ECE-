{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Victor Papin - TD 2 - Data & IA - ING 5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0e3674a-0e18-49ea-bf42-801c93d106a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploratory Data Analysis with Pyspark and Spark SQL\n",
    "\n",
    "The following notebook utilizes New York City taxi data from [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Load and explore nyc taxi data from january 0f 2019. The exercises can be executed using pyspark or spark sql ( a subset of the questions will be re-answered using the language not chosen for the  main work).\n",
    "- Load the zone lookup table to answer the questions about the nyc boroughs.  \n",
    "- Load nyc taxi data from January of 2025 and compare data.  \n",
    "- With any remaining time, work on the where to go from here section.  \n",
    "- Lab due date is TBD ( due dates will be updated in the readme for the class repo )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f445f856-1ce8-4dc7-88e2-90d7aed027ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbutils\n",
    "import spark\n",
    "\n",
    "# Define the name of the new catalog\n",
    "catalog = 'taxi_eda_db'\n",
    "\n",
    "# define variables for the trips data\n",
    "schema = 'yellow_taxi_trips'\n",
    "volume = 'data'\n",
    "file_name = 'yellow_tripdata_2019-01.parquet'\n",
    "table_name = 'tbl_yellow_taxi_trips'\n",
    "path_volume = '/Volumes/' + catalog + \"/\" + schema + '/' + volume\n",
    "path_table =  catalog + \".\" + schema\n",
    "download_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "545e8186-b880-4cb8-a5f6-d73e4b407a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the catalog/schema/volume\n",
    "spark.sql('create catalog if not exists ' + catalog)\n",
    "spark.sql('create schema if not exists ' + catalog + '.' + schema)\n",
    "spark.sql('create volume if not exists ' + catalog + '.' + schema + '.' + volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8576a8-5cbe-43f6-afc0-a0c844b66755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the data\n",
    "dbutils.fs.cp(f\"{download_url}\", f\"{path_volume}\" + \"/\" + f\"{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be82981b-8af8-4821-a20f-e1c5ffbde15b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the dataframe\n",
    "df_trips = spark.read.parquet(f\"{path_volume}/{file_name}\",\n",
    "  header=True,\n",
    "  inferSchema=True,\n",
    "  sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bffe652-71fd-404e-913d-7f368427d7c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the dataframe\n",
    "df_trips.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95c3add9-ff1a-4570-aa20-c65a4dc0514f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lab\n",
    "\n",
    "### Part 1\n",
    "This section can be completed either using pyspark commands or sql commands ( There will be a section after in which a self-chosen subset of the questions are re-answered using the language not used for the main section. i.e. if pyspark is chosen for the main lab, sql should be used to repeat some of the questions. )\n",
    "\n",
    "- Add a column that creates a unique key to identify each record in order to answer questions about individual trips\n",
    "- Which trip has the highest passanger count\n",
    "- What is the Average passanger count\n",
    "- Shortest/longest trip by distance? by time?.\n",
    "- busiest day/slowest single day\n",
    "- busiest/slowest time of day ( you may want to bucket these by hour or create timess such as morning, afternoon, evening, late night )\n",
    "- On average which day of the week is slowest/busiest\n",
    "- Does trip distance or num passangers affect tip amount\n",
    "- What was the highest \"extra\" charge and which trip\n",
    "- Are there any datapoints that seem to be strange/outliers (make sure to explain your reasoning in a markdown cell)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa2f5628-1cbd-4195-98f2-c996ca243535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 2\n",
    "\n",
    "- Using the code for loading the first dataset as an example, load in the taxi zone lookup and answer the following questions\n",
    "- which borough had most pickups? dropoffs?\n",
    "- what are the busy/slow times by borough \n",
    "- what are the busiest days of the week by borough?\n",
    "- what is the average trip distance by borough?\n",
    "- what is the average trip fare by borough?\n",
    "- highest/lowest faire amounts for a trip, what burough is associated with the each\n",
    "- load the dataset from the most recently available january, is there a change to any of the average metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70500fa8-1b6f-48b8-8fec-4bb988a9f183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 3\n",
    "\n",
    "- choose 3 questions from above and re-answer them using the language you did not use for the main notebook . (i.e - if you completed the exercise in python, redo 3 questions in pure sql) . at least one of the questions to be redone must involve a join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4376b74-d167-4516-a30c-0f28fbed0430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Part 4\n",
    "\n",
    "As of spark v4 dataframes have native visualization support. Choose at least 3 questions from above and provide visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d71458-86fd-4602-aba5-cca2e9a7054f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Where to go from here\n",
    "\n",
    "- Continue building the dataset by loading in more data, start by completing the data for 2019 and calculating the busiest season (fall, winter, spring, summer)\n",
    "- Explore a dataset/datasets of your choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NYC Taxi EDA\").getOrCreate()\n",
    "\n",
    "yellow_tripdata_path = download_url\n",
    "zone_lookup_path = 'taxi_zone_lookup.csv'\n",
    "\n",
    "df_trips = spark.read.parquet(yellow_tripdata_path)\n",
    "df_zones = spark.read.csv(zone_lookup_path, header=True, inferSchema=True)\n",
    "\n",
    "df_trips.cache()\n",
    "df_trips.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_trips = df_trips.withColumn(\"trip_id\", F.monotonically_increasing_id())\n",
    "df_trips.orderBy(F.desc(\"passenger_count\")).select(\"trip_id\",\"passenger_count\",\"trip_distance\",\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\").show(1)\n",
    "df_trips.agg(F.avg(\"passenger_count\")).show()\n",
    "df_trips.orderBy(\"trip_distance\").select(\"trip_id\",\"trip_distance\",\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\").show(1)\n",
    "df_trips.orderBy(F.desc(\"trip_distance\")).select(\"trip_id\",\"trip_distance\",\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\").show(1)\n",
    "df_trips = df_trips.withColumn(\"trip_duration_minutes\",(F.unix_timestamp(\"tpep_dropoff_datetime\")-F.unix_timestamp(\"tpep_pickup_datetime\"))/60)\n",
    "df_trips.orderBy(\"trip_duration_minutes\").select(\"trip_id\",\"trip_duration_minutes\").show(1)\n",
    "df_trips.orderBy(F.desc(\"trip_duration_minutes\")).select(\"trip_id\",\"trip_duration_minutes\").show(1)\n",
    "df_by_date = df_trips.groupBy(F.to_date(\"tpep_pickup_datetime\").alias(\"date\")).count()\n",
    "df_by_date.orderBy(F.desc(\"count\")).show(1)\n",
    "df_by_date.orderBy(\"count\").show(1)\n",
    "df_trips = df_trips.withColumn(\"hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "df_trips = df_trips.withColumn(\"time_of_day\", F.when((df_trips.hour >= 5) & (df_trips.hour < 12), \"morning\").when((df_trips.hour >= 12) & (df_trips.hour < 17), \"afternoon\").when((df_trips.hour >= 17) & (df_trips.hour < 21), \"evening\").otherwise(\"late_night\"))\n",
    "df_trips.groupBy(\"time_of_day\").count().orderBy(F.desc(\"count\")).show()\n",
    "df_trips.groupBy(\"time_of_day\").count().orderBy(\"count\").show()\n",
    "df_by_dow = df_trips.groupBy(F.date_format(\"tpep_pickup_datetime\",\"E\").alias(\"day_of_week\")).count()\n",
    "df_by_dow.orderBy(F.desc(\"count\")).show()\n",
    "df_trips.stat.corr(\"trip_distance\",\"tip_amount\")\n",
    "df_trips.stat.corr(\"passenger_count\",\"tip_amount\")\n",
    "df_trips.orderBy(F.desc(\"extra\")).select(\"trip_id\",\"extra\",\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\").show(1)\n",
    "quantiles = df_trips.approxQuantile(\"trip_distance\",[0.25,0.75],0.05)\n",
    "iqr = quantiles[1] - quantiles[0]\n",
    "lower_bound = quantiles[0] - 1.5*iqr\n",
    "upper_bound = quantiles[1] + 1.5*iqr\n",
    "df_outliers = df_trips.filter((df_trips.trip_distance < lower_bound) | (df_trips.trip_distance > upper_bound))\n",
    "df_outliers.count()df_trips.createOrReplaceTempView(\"trips\")\n",
    "df_zones.createOrReplaceTempView(\"zones\")\n",
    "\n",
    "pickups = df_zones.withColumnRenamed(\"LocationID\",\"PULocationID\").withColumnRenamed(\"Borough\",\"pickup_borough\")\n",
    "dropoffs = df_zones.withColumnRenamed(\"LocationID\",\"DOLocationID\").withColumnRenamed(\"Borough\",\"dropoff_borough\")\n",
    "df_with_zones = df_trips.join(pickups, on=\"PULocationID\", how=\"left\").join(dropoffs, on=\"DOLocationID\", how=\"left\")\n",
    "\n",
    "df_with_zones.groupBy(\"pickup_borough\").count().orderBy(F.desc(\"count\")).show(1)\n",
    "df_with_zones.groupBy(\"dropoff_borough\").count().orderBy(F.desc(\"count\")).show(1)\n",
    "df_with_zones.groupBy(\"pickup_borough\",\"time_of_day\").count().orderBy(F.desc(\"count\")).show()\n",
    "df_with_zones.groupBy(\"pickup_borough\", F.date_format(\"tpep_pickup_datetime\",\"E\").alias(\"day_of_week\")).count().orderBy(F.desc(\"count\")).show()\n",
    "df_with_zones.groupBy(\"pickup_borough\").agg(F.avg(\"trip_distance\").alias(\"avg_distance\")).orderBy(F.desc(\"avg_distance\")).show()\n",
    "df_with_zones.groupBy(\"pickup_borough\").agg(F.avg(\"total_amount\").alias(\"avg_fare\")).orderBy(F.desc(\"avg_fare\")).show()\n",
    "highest_fare = df_with_zones.orderBy(F.desc(\"total_amount\")).select(\"pickup_borough\",\"total_amount\",\"trip_id\").limit(1)\n",
    "lowest_fare = df_with_zones.orderBy(\"total_amount\").select(\"pickup_borough\",\"total_amount\",\"trip_id\").limit(1)\n",
    "highest_fare.show()\n",
    "lowest_fare.show()\n",
    "\n",
    "recent_path = 'path/to/yellow_tripdata_2025-01.parquet'\n",
    "df_recent = spark.read.parquet(recent_path)\n",
    "df_recent = df_recent.withColumn(\"trip_duration_minutes\",(F.unix_timestamp(\"tpep_dropoff_datetime\")-F.unix_timestamp(\"tpep_pickup_datetime\"))/60)\n",
    "recent_avg_distance = df_recent.agg(F.avg(\"trip_distance\")).first()[0]\n",
    "recent_avg_passengers = df_recent.agg(F.avg(\"passenger_count\")).first()[0]\n",
    "recent_avg_fare = df_recent.agg(F.avg(\"total_amount\")).first()[0]\n",
    "recent_avg_distance, recent_avg_passengers, recent_avg_farespark.sql(\"SELECT z.Borough AS pickup_borough, COUNT(*) AS pickups FROM trips t JOIN zones z ON t.PULocationID = z.LocationID GROUP BY z.Borough ORDER BY pickups DESC LIMIT 1\").show()\n",
    "spark.sql(\"SELECT z.Borough AS pickup_borough, AVG(t.trip_distance) AS avg_distance FROM trips t JOIN zones z ON t.PULocationID = z.LocationID GROUP BY z.Borough ORDER BY avg_distance DESC\").show()\n",
    "spark.sql(\"SELECT t.trip_id, z.Borough AS pickup_borough, t.total_amount FROM trips t JOIN zones z ON t.PULocationID = z.LocationID ORDER BY t.total_amount DESC LIMIT 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT z.Borough AS pickup_borough, COUNT(*) AS pickups FROM trips t JOIN zones z ON t.PULocationID = z.LocationID GROUP BY z.Borough ORDER BY pickups DESC LIMIT 1\").show()\n",
    "spark.sql(\"SELECT z.Borough AS pickup_borough, AVG(t.trip_distance) AS avg_distance FROM trips t JOIN zones z ON t.PULocationID = z.LocationID GROUP BY z.Borough ORDER BY avg_distance DESC\").show()\n",
    "spark.sql(\"SELECT t.trip_id, z.Borough AS pickup_borough, t.total_amount FROM trips t JOIN zones z ON t.PULocationID = z.LocationID ORDER BY t.total_amount DESC LIMIT 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "eda_nyc_taxi",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
